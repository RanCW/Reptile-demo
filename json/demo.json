{"paging":{"is_end":false,"is_start":true,"next":"https://zhuanlan.zhihu.com/api/articles/61583602/recommendation?include=data%5B%2A%5D.article.column\u0026limit=12\u0026offset=12","previous":"https://zhuanlan.zhihu.com/api/articles/61583602/recommendation?include=data%5B%2A%5D.article.column\u0026limit=12\u0026offset=0"},"data":[{"article":{"id":69288675,"title":"用Node.js爬取动态网页，这可能是最简洁的方式","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/69288675","image_url":"https://pic4.zhimg.com/v2-5657809679bad9ac3816ae720de24607_720w.jpg?source=172ae18b","title_image":"https://pic3.zhimg.com/v2-5657809679bad9ac3816ae720de24607_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic4.zhimg.com/v2-bad7b56200918b4f30e7e5df96856017_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2884\" data-rawheight=\"1368\" data-watermark=\"watermark\" data-original-src=\"v2-bad7b56200918b4f30e7e5df96856017\" data-watermark-src=\"v2-e195097d17a2bea167d720d97fa7d75e\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic4.zhimg.com/v2-bad7b56200918b4f30e7e5df96856017_r.png\"/\u003e现在网页大多是动态网页，如果只是单纯地通过爬取网页HTML文件，根本爬取不到需要后续加载的商品价格或图片等重要信息，更别谈那些丧心病狂的登录限制，对于小爬虫来说，去分析那些复杂的脚本得不偿失，更别谈网站还会与时俱进地更新，好不容易破解了，人家…","created":1560598244,"updated":1607763902,"author":{"avatar_url_template":"https://pic1.zhimg.com/v2-138cebf1520eded6ce1fc523c31773ee.jpg?source=172ae18b","uid":"884410883143532544","user_type":"people","url_token":"beilu007","id":"c2f3baabd84744974869d8ecd1a3b208","description":"","name":"林城","is_advertiser":false,"headline":"","gender":1,"url":"/people/c2f3baabd84744974869d8ecd1a3b208","avatar_url":"https://pic1.zhimg.com/v2-138cebf1520eded6ce1fc523c31773ee_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":124,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":51226507,"title":"Node.js实现简单网页数据抓取","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/51226507","image_url":"https://pic1.zhimg.com/v2-d5c67c6d6d5b4d346d9038a9447d9872_720w.jpg?source=172ae18b","title_image":"https://pic1.zhimg.com/v2-d5c67c6d6d5b4d346d9038a9447d9872_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic4.zhimg.com/v2-9631bdcc318e3d9b96f98bdecad4488b_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"855\" data-rawheight=\"195\" data-watermark=\"watermark\" data-original-src=\"v2-9631bdcc318e3d9b96f98bdecad4488b\" data-watermark-src=\"v2-dac96b9470d8e0a057f2cbff934dea56\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic4.zhimg.com/v2-9631bdcc318e3d9b96f98bdecad4488b_r.jpg\"/\u003e由于自己的小项目需要后台支持，所以前段时间学习了下 Node.js。简单介绍下 Node.js，Node.js 是一个基于 JavaScript 语言和 V8 引擎运行在服务端的并发平台，在 Node.js 出现以后，使得 JavaScrip 不光能写前端的动态效果，交互效果，还能写 web 服务器，…","created":1543582420,"updated":1543582420,"author":{"avatar_url_template":"https://pic1.zhimg.com/v2-7b1df4406e9b41155677d3974201b883.jpg?source=172ae18b","uid":"706548679753879552","user_type":"people","url_token":"zhang-teng-jin-58","id":"cde4883392509cc9e7c61f9c01e493db","description":"","name":"青忆同学","is_advertiser":false,"headline":"认真生活 快乐工作","gender":1,"url":"/people/cde4883392509cc9e7c61f9c01e493db","avatar_url":"https://pic4.zhimg.com/v2-7b1df4406e9b41155677d3974201b883_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":12,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":26463840,"title":"基于 Node.js 的声明式可监控爬虫网络","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/26463840","image_url":"https://pic3.zhimg.com/v2-a0494a93be0d0f0847b95cc8b55073bd_720w.jpg?source=172ae18b","title_image":"https://pic2.zhimg.com/v2-a0494a93be0d0f0847b95cc8b55073bd_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic2.zhimg.com/v2-e5b6bd6e5171c14a13b66d46a11c622d_200x112.png\" data-rawwidth=\"1130\" data-rawheight=\"503\" data-original=\"https://pic2.zhimg.com/v2-e5b6bd6e5171c14a13b66d46a11c622d_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"/\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/26463840\" class=\"internal\"\u003e基于 Node.js 的声明式可监控爬虫网络\u003c/a\u003e从属于笔者的，记述了笔者重构我司简单爬虫过程中构建简单的爬虫框架的思想与实现，代码参考\u003ca href=\"https://link.zhihu.com/?target=https%3A//parg.co/bR2\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e基于 Node.js 的声明式可监控爬虫网络爬虫是数据抓取的重要手段之一，而以 \u003ca href=\"https://link.zhihu.com/?target=https%3A//doc.scrapy.org/en/latest/intro/tutorial.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eScrapy\u003c/a\u003e、\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/yasserg/crawler4j\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eCrawler4j\u003c/a\u003e、\u003ca href=\"https://link.zhihu.com/?target=http%3A//nutch.apache.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eNutch\u003c/a\u003e 为代表的开源框架能…","created":1492612221,"updated":1492612220,"author":{"avatar_url_template":"https://pic1.zhimg.com/v2-a627d79d2ed03fe6f83a11743a18d909.jpg?source=172ae18b","uid":"57299368411136","user_type":"people","url_token":"wxyyxc1992","id":"ed4cd6b92a003a0ce8e801ae74196e19","description":"https://segmentfault.com/u/wxyyxc1992\n\nhttps://github.com/wxyyxc1992","name":"王下邀月熊","is_advertiser":false,"headline":"公众号：某熊的技术之路","gender":1,"url":"/people/ed4cd6b92a003a0ce8e801ae74196e19","avatar_url":"https://pic4.zhimg.com/v2-a627d79d2ed03fe6f83a11743a18d909_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":75,"column":{"description":"让知识在它该在的地方","intro":"我的技术世界，Web前端、服务端应用架构、数据挖掘","url_token":"wxyyxc1992","id":"wxyyxc1992","accept_submission":false,"title":"某熊的技术之路","url":"https://zhuanlan.zhihu.com/wxyyxc1992","comment_permission":"all","created":1481802498,"updated":1591361197,"image_url":"https://pic4.zhimg.com/v2-3c441415846977fe344039486f932e19_720w.jpg?source=172ae18b","author":{"avatar_url_template":"https://pic1.zhimg.com/v2-a627d79d2ed03fe6f83a11743a18d909.jpg?source=172ae18b","uid":"57299368411136","user_type":"people","url_token":"wxyyxc1992","id":"ed4cd6b92a003a0ce8e801ae74196e19","description":"https://segmentfault.com/u/wxyyxc1992\n\nhttps://github.com/wxyyxc1992","name":"王下邀月熊","is_advertiser":false,"headline":"公众号：某熊的技术之路","gender":1,"url":"/people/ed4cd6b92a003a0ce8e801ae74196e19","avatar_url":"https://pic1.zhimg.com/v2-a627d79d2ed03fe6f83a11743a18d909_l.jpg?source=172ae18b","is_org":false,"type":"people"},"type":"column"},"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":61419719,"title":"使用 nodejs 写爬虫(一): 常用模块和 js 语法","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/61419719","image_url":"https://pic4.zhimg.com/v2-a64410eaedc141a444926d27c910264a_720w.jpg?source=172ae18b","title_image":"https://pic1.zhimg.com/v2-a64410eaedc141a444926d27c910264a_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic4.zhimg.com/v2-c2cd797f6614b61d1297554bd4d6f25f_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"178\" data-watermark=\"\" data-original-src=\"\" data-watermark-src=\"\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic4.zhimg.com/v2-c2cd797f6614b61d1297554bd4d6f25f_r.png\"/\u003e本篇是使用 nodejs 写爬虫系列教程的第一篇, 介绍了使用 nodejs 写爬虫过程中常用的模块和一些必须掌握的 js 语法 \u0026lt;!-- more --\u0026gt; \u003cb\u003e常用模块\u003c/b\u003e常用模块有以下几个: \u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/jprichardson/node-fs-extra\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003efs-extra\u003c/a\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/visionmedia/superagent\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003esuperagent\u003c/a\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/cheeriojs/cheerio\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003echeerio\u003c/a\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/log4js-node/log4js-node\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003elog4js\u003c/a\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/sequelize/sequelize\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003esequelize\u003c/a\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/chalk/chalk\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003echalk\u003c/a\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/GoogleChrome/puppeteer\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003epuppeteer\u003c/a\u003e\u003cb\u003efs-extra\u003c/b\u003e使用 async/await 的前提是必…","created":1554358835,"updated":1554811514,"author":{"avatar_url_template":"https://pic4.zhimg.com/v2-a2122d61cb1b49987bed5ff8422720a6.jpg?source=172ae18b","uid":"793248355764539392","user_type":"people","url_token":"yu-teng-jing","id":"d2ac77c3867351c2a632312b2f417e24","description":"github: https://github.com/tjx666\nblog: http://www.lyreal666.com\nemail: ytj2713151713@gmail.com\n","name":"余腾靖","is_advertiser":false,"headline":"会一点前端的 flutter 开发","gender":1,"url":"/people/d2ac77c3867351c2a632312b2f417e24","avatar_url":"https://pic1.zhimg.com/v2-a2122d61cb1b49987bed5ff8422720a6_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":3,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":94451941,"title":"记一次node爬虫经历，手把手教你爬虫","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/94451941","image_url":"https://pic4.zhimg.com/v2-d5c67c6d6d5b4d346d9038a9447d9872_720w.jpg?source=172ae18b","title_image":"https://pic2.zhimg.com/v2-d5c67c6d6d5b4d346d9038a9447d9872_720w.jpg?source=172ae18b","excerpt":"今天业务突然来了个爬虫业务，爬出来的数据以Excel的形式导出，下班前一个小时开始做，加班一个小时就做好了。因为太久没做爬虫了！做这个需求都是很兴奋！需求说明访问网站（循环）获取页面指定数据源根据页面数据源再（循环）访问详情数据记录详情数据，…","created":1575094628,"updated":1575094713,"author":{"avatar_url_template":"https://pic4.zhimg.com/v2-ade717625e932bb53e769765ccb70f5c.jpg?source=172ae18b","uid":"751221478875201536","user_type":"people","url_token":"scottjeremy","id":"707d5f1b5f4ca96329c01aa2afd15608","description":"一直在追赶，从未敢怠慢！","name":"Scott-Jeremy","is_advertiser":false,"headline":"一名不只会写前端的程序员","gender":1,"url":"/people/707d5f1b5f4ca96329c01aa2afd15608","avatar_url":"https://pic2.zhimg.com/v2-ade717625e932bb53e769765ccb70f5c_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":4,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":26556429,"title":"node.js简单爬虫扒取妹子图","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/26556429","image_url":"","title_image":"","excerpt":"\u003cimg src=\"https://pic3.zhimg.com/v2-713ad51ab0740d1c34d002f37a9c2fde_200x112.png\" data-rawwidth=\"591\" data-rawheight=\"115\" data-original=\"https://pic3.zhimg.com/v2-713ad51ab0740d1c34d002f37a9c2fde_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"/\u003e初学node.js，想着用node做点什么，最后决定写一个简单的爬虫，并且把扒取目标定在了 \u003ca href=\"https://link.zhihu.com/?target=http%3A//www.mmjpg.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e妹子图\u003c/a\u003e。爬虫用到了两个第三方模块，分别是 \u003ca href=\"https://link.zhihu.com/?target=https%3A//segmentfault.com/a/1190000003968699\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003echeerio\u003c/a\u003e 和 \u003ca href=\"https://link.zhihu.com/?target=https%3A//segmentfault.com/a/1190000000385867\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003erequest\u003c/a\u003e ，没有接触过的同学可以点进入先了解个大概。 1.目录结构文件目录很简单，一个js文件入口，一个用来存放…","created":1493094703,"updated":1493094703,"author":{"avatar_url_template":"https://pic4.zhimg.com/v2-272ed529bb8102957b8011fdd3cc7994.jpg?source=172ae18b","uid":"741056870231310336","user_type":"people","url_token":"gu-jia-ming-zhu","id":"eae7f2fe583cc3d4c31bc64fb8ed7f16","description":"只会JavaScript，只会console.log","name":"LLzzZZ","is_advertiser":false,"headline":"他们总说我瓜，其实我一点都不瓜....大多数的时候，我都机智的一逼....","gender":1,"url":"/people/eae7f2fe583cc3d4c31bc64fb8ed7f16","avatar_url":"https://pic2.zhimg.com/v2-272ed529bb8102957b8011fdd3cc7994_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":28,"column":{"description":"一只前端开发小白记录学习心得的地方","intro":"随时记录分享一些前端小知识","url_token":"pipipi-pikachu","id":"pipipi-pikachu","accept_submission":true,"title":"前端随记","url":"https://zhuanlan.zhihu.com/pipipi-pikachu","comment_permission":"all","created":1487588055,"updated":1600677945,"image_url":"https://pic2.zhimg.com/v2-ad27876bc151dea437057f2050abd65c_720w.jpg?source=172ae18b","author":{"avatar_url_template":"https://pic1.zhimg.com/v2-272ed529bb8102957b8011fdd3cc7994.jpg?source=172ae18b","uid":"741056870231310336","user_type":"people","url_token":"gu-jia-ming-zhu","id":"eae7f2fe583cc3d4c31bc64fb8ed7f16","description":"只会JavaScript，只会console.log","name":"LLzzZZ","is_advertiser":false,"headline":"他们总说我瓜，其实我一点都不瓜....大多数的时候，我都机智的一逼....","gender":1,"url":"/people/eae7f2fe583cc3d4c31bc64fb8ed7f16","avatar_url":"https://pic1.zhimg.com/v2-272ed529bb8102957b8011fdd3cc7994_l.jpg?source=172ae18b","is_org":false,"type":"people"},"type":"column"},"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":38347588,"title":"爬虫知多少-（NodeJS 爬虫）","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/38347588","image_url":"https://pic1.zhimg.com/v2-b3501d2ff5142722fdb3c2368a31ce09_720w.jpg?source=172ae18b","title_image":"https://pic1.zhimg.com/v2-b3501d2ff5142722fdb3c2368a31ce09_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic1.zhimg.com/v2-6ac6f619b89323d51c56aa1a00e0fda8_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"540\" data-watermark=\"watermark\" data-original-src=\"v2-6ac6f619b89323d51c56aa1a00e0fda8\" data-watermark-src=\"v2-46c187b4385eaf500d2cc2c96e1b8cac\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic1.zhimg.com/v2-6ac6f619b89323d51c56aa1a00e0fda8_r.jpg\"/\u003e不久前在公司做了一个关于爬虫的分享，简单介绍了网络爬虫的基础知识、爬虫的运作方式、抓取策略、攻防方式以及如何使用 NodeJS 进行爬虫开发， 在这里分享给各位同学分享一下~ 一、爬虫简介 二、爬虫的运作方式 三、抓取策略 （1）深度优先搜索 （2）广度…","created":1529591633,"updated":1617963669,"author":{"avatar_url_template":"https://pic4.zhimg.com/v2-ea39747035cd4307973a3e031b5acb65.jpg?source=172ae18b","uid":"635060684149362688","user_type":"people","url_token":"xie-zi-65-35-10","id":"f3e598ed1fd15ff728570df0d77f5005","description":"容易走的路，都是下坡路。\n公众号：「皮蛋菌丶」","name":"皮蛋菌丶","is_advertiser":false,"headline":"前端 · 理财 · 养了只柯基叫皮蛋","gender":1,"url":"/people/f3e598ed1fd15ff728570df0d77f5005","avatar_url":"https://pic4.zhimg.com/v2-ea39747035cd4307973a3e031b5acb65_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":5,"column":{"description":"","intro":"","url_token":"WeCode365","id":"WeCode365","accept_submission":true,"title":"WeCode365","url":"https://zhuanlan.zhihu.com/WeCode365","comment_permission":"all","created":1478492107,"updated":1591400350,"image_url":"https://pic2.zhimg.com/v2-003721bbc0ee045f77b7b38e9b7f532a_720w.jpg?source=172ae18b","author":{"avatar_url_template":"https://pic1.zhimg.com/v2-ea39747035cd4307973a3e031b5acb65.jpg?source=172ae18b","uid":"635060684149362688","user_type":"people","url_token":"xie-zi-65-35-10","id":"f3e598ed1fd15ff728570df0d77f5005","description":"容易走的路，都是下坡路。\n公众号：「皮蛋菌丶」","name":"皮蛋菌丶","is_advertiser":false,"headline":"前端 · 理财 · 养了只柯基叫皮蛋","gender":1,"url":"/people/f3e598ed1fd15ff728570df0d77f5005","avatar_url":"https://pic1.zhimg.com/v2-ea39747035cd4307973a3e031b5acb65_l.jpg?source=172ae18b","is_org":false,"type":"people"},"type":"column"},"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":72344793,"title":"关于爬虫本地JS Hook的研究","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/72344793","image_url":"","title_image":"","excerpt":"\u003cimg src=\"https://pic3.zhimg.com/v2-811a65d8e1dc4bc6b495e3cf783b9546_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1700\" data-rawheight=\"376\" data-watermark=\"original\" data-original-src=\"v2-811a65d8e1dc4bc6b495e3cf783b9546\" data-watermark-src=\"v2-e3c48758bcbf7af8ea6eee3a2e9de013\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic3.zhimg.com/v2-811a65d8e1dc4bc6b495e3cf783b9546_r.png\"/\u003e0x00 背景介绍最早的爬虫，只需要能够从服务端获取到HTML代码，进行分析即可，随着Web2.0的普及，越来越多的网站都必须JavaScript解析之后才能正常显示。因此这也对爬虫提出了新的要求，当前前人们已经在爬虫中集成Webkit等框架来满足这样的需求。 本文将从…","created":1562319163,"updated":1562319166,"author":{"avatar_url_template":"https://pic1.zhimg.com/v2-31e47f21cb0d425f7dfdf801537157de.jpg?source=172ae18b","uid":"984835495964078080","user_type":"organization","url_token":"hua-wei-yun-ji-zhu-zhai-ji-di","id":"a97ebcb387aaf4dec46f5179b8a71ec9","description":"华为云开发者社区，提供全面深入的云计算前景分析、丰富的技术干货、程序样例，分享华为云前沿资讯动态，方便开发者快速成长与发展，欢迎提问、互动，多方位了解云计算！","name":"华为云开发者社区","is_advertiser":false,"headline":"来自华为云的新鲜技术分享","gender":1,"url":"/org/a97ebcb387aaf4dec46f5179b8a71ec9","avatar_url":"https://pic1.zhimg.com/v2-31e47f21cb0d425f7dfdf801537157de_l.jpg?source=172ae18b","is_org":true,"type":"people","badge":[{"type":"identity","topics":[],"description":"已认证的官方帐号"}],"badge_v2":{"title":"已认证的官方帐号","merged_badges":[{"type":"identity","detail_type":"identity","title":"认证","description":"已认证的官方帐号","url":"https://www.zhihu.com/account/verification/intro","sources":[],"icon":"","night_icon":""}],"detail_badges":[{"type":"identity","detail_type":"identity_org","title":"已认证的官方帐号","description":"已认证的官方帐号","url":"https://www.zhihu.com/account/verification/intro","sources":[],"icon":"https://pic2.zhimg.com/v2-235258cecb8a0f184c4d38483cd6f6b6_l.png","night_icon":"https://pic3.zhimg.com/v2-45e870b8f0982bcd7537ea4627afbd00_l.png"}],"icon":"https://pic3.zhimg.com/v2-235258cecb8a0f184c4d38483cd6f6b6_l.png","night_icon":"https://pic4.zhimg.com/v2-45e870b8f0982bcd7537ea4627afbd00_l.png"}},"comment_permission":"censor","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":8,"column":{"description":"程序员的自由讨论空间，关于技术、生活和社会","intro":"欢迎投稿、交流","url_token":"c_1016019161834774528","id":"c_1016019161834774528","accept_submission":true,"title":"程序员之家","url":"https://zhuanlan.zhihu.com/c_1016019161834774528","comment_permission":"all","created":1535017844,"updated":1599158324,"image_url":"https://pic2.zhimg.com/v2-dd3b579214bf8adfb9c0b7e7defb8dda_720w.jpg?source=172ae18b","author":{"avatar_url_template":"https://pic2.zhimg.com/v2-31e47f21cb0d425f7dfdf801537157de.jpg?source=172ae18b","uid":"984835495964078080","user_type":"organization","url_token":"hua-wei-yun-ji-zhu-zhai-ji-di","id":"a97ebcb387aaf4dec46f5179b8a71ec9","description":"华为云开发者社区，提供全面深入的云计算前景分析、丰富的技术干货、程序样例，分享华为云前沿资讯动态，方便开发者快速成长与发展，欢迎提问、互动，多方位了解云计算！","name":"华为云开发者社区","is_advertiser":false,"headline":"来自华为云的新鲜技术分享","gender":1,"url":"/org/a97ebcb387aaf4dec46f5179b8a71ec9","avatar_url":"https://pic4.zhimg.com/v2-31e47f21cb0d425f7dfdf801537157de_l.jpg?source=172ae18b","is_org":true,"type":"people"},"type":"column"},"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":64005248,"title":"爬虫被封怎么办？用Node.js构建一个私人IP代理池","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/64005248","image_url":"https://pic1.zhimg.com/v2-70e177991d1d522f044196bb17ea88e7_720w.jpg?source=172ae18b","title_image":"https://pic4.zhimg.com/v2-70e177991d1d522f044196bb17ea88e7_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic4.zhimg.com/v2-70eff1b6e824d79bb481c2239422d197_200x112.png\" data-rawwidth=\"1798\" data-rawheight=\"1572\" data-size=\"normal\" data-caption=\"\" data-watermark=\"watermark\" data-original-src=\"v2-70eff1b6e824d79bb481c2239422d197\" data-watermark-src=\"v2-9b682897e060cbe1f2d73984623af872\" data-private-watermark-src=\"\" data-original=\"https://pic4.zhimg.com/v2-70eff1b6e824d79bb481c2239422d197_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"/\u003e还记得刚学爬虫的时候，选了一个美女网站来练手，效率极高，看到什么都想爬下来。爬得正高兴呢，出现了一连串错误信息，查看后发现因为爬取太过频繁，被网站封了ip，那时起就有了构建代理ip池的念头。 网上搜索一下代理ip就会发现有很多网站提供，但是稳定…","created":1556372897,"updated":1560923774,"author":{"avatar_url_template":"https://pic2.zhimg.com/v2-138cebf1520eded6ce1fc523c31773ee.jpg?source=172ae18b","uid":"884410883143532544","user_type":"people","url_token":"beilu007","id":"c2f3baabd84744974869d8ecd1a3b208","description":"","name":"林城","is_advertiser":false,"headline":"","gender":1,"url":"/people/c2f3baabd84744974869d8ecd1a3b208","avatar_url":"https://pic4.zhimg.com/v2-138cebf1520eded6ce1fc523c31773ee_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":100,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":67549785,"title":"摆脱js回调地狱，Async/Await实用指南，完美实现同步操作","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/67549785","image_url":"https://pic4.zhimg.com/v2-59f8e64b01992b5a5ae670d815e1307c_720w.jpg?source=172ae18b","title_image":"https://pic4.zhimg.com/v2-59f8e64b01992b5a5ae670d815e1307c_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic4.zhimg.com/v2-1b8385a2b8a3cbd49675041a8f852fbf_200x112.png\" data-rawwidth=\"1372\" data-rawheight=\"922\" data-size=\"normal\" data-caption=\"\" data-watermark=\"watermark\" data-original-src=\"v2-1b8385a2b8a3cbd49675041a8f852fbf\" data-watermark-src=\"v2-d8863a7c259e60f048d1e70f664eecdb\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic4.zhimg.com/v2-1b8385a2b8a3cbd49675041a8f852fbf_r.png\"/\u003e在JavaScript里，所有的操作都是通过异步完成，不需要像python一样按顺序执行，非常容易因为某个函数执行速度过慢导致整个程序阻塞，从效率上来说远远低于JavaScript的原生异步机制。当然，python也可以实现异步功能，但是会麻烦不少，尤其对于一些代码量少…","created":1559217285,"updated":1560758335,"author":{"avatar_url_template":"https://pic4.zhimg.com/v2-138cebf1520eded6ce1fc523c31773ee.jpg?source=172ae18b","uid":"884410883143532544","user_type":"people","url_token":"beilu007","id":"c2f3baabd84744974869d8ecd1a3b208","description":"","name":"林城","is_advertiser":false,"headline":"","gender":1,"url":"/people/c2f3baabd84744974869d8ecd1a3b208","avatar_url":"https://pic2.zhimg.com/v2-138cebf1520eded6ce1fc523c31773ee_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":20,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":339123494,"title":"高阶爬虫笔记（一）——与前端斗智斗勇","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/339123494","image_url":"","title_image":"","excerpt":"\u003cimg src=\"https://pic4.zhimg.com/v2-37e1da23db4a49b39facbf832855c6e3_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"937\" data-watermark=\"watermark\" data-original-src=\"v2-37e1da23db4a49b39facbf832855c6e3\" data-watermark-src=\"v2-f50fb4a44fa27b6a6f3ef0b93edf92ff\" data-private-watermark-src=\"\" data-original=\"https://pic4.zhimg.com/v2-37e1da23db4a49b39facbf832855c6e3_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"/\u003e写在开头为什么会写这篇笔记：最近干活的时候需要爬取一个知名电商平台的店铺后台数据，研究多日积累了不少经验，故写此文以备忘。csdn充斥着大量垃圾爬虫资源，有些资源收费还非常高。即便这些别人写好的爬虫可以使用，使用者也通常是知其然而不知其所以然…","created":1608785396,"updated":1608785396,"author":{"avatar_url_template":"https://pic3.zhimg.com/v2-9ae24df43cdd60f55dc4fd61277dc52f.jpg?source=172ae18b","uid":"867830262422204416","user_type":"people","url_token":"dragonlore","id":"cb9ff812937f67c92733725c0279eb20","description":"本科，学生","name":"dragonlore","is_advertiser":false,"headline":"大数据与供应链管理","gender":1,"url":"/people/cb9ff812937f67c92733725c0279eb20","avatar_url":"https://pic1.zhimg.com/v2-9ae24df43cdd60f55dc4fd61277dc52f_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":9,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false},{"article":{"id":104262623,"title":"（原创）Node.JS实战58：写一套反爬虫系统！","type":"article","article_type":"normal","excerpt_title":"","url":"https://zhuanlan.zhihu.com/p/104262623","image_url":"https://pic4.zhimg.com/v2-b78a81dbdb83e86c2b756be601dab621_720w.jpg?source=172ae18b","title_image":"https://pic4.zhimg.com/v2-b78a81dbdb83e86c2b756be601dab621_720w.jpg?source=172ae18b","excerpt":"\u003cimg src=\"https://pic2.zhimg.com/v2-103d03ad86229cd090914352f642c605_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"828\" data-rawheight=\"161\" data-watermark=\"original\" data-original-src=\"v2-103d03ad86229cd090914352f642c605\" data-watermark-src=\"v2-c50c123dec52f802bb91e6ad5f8f0a9a\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https://pic2.zhimg.com/v2-103d03ad86229cd090914352f642c605_r.jpg\"/\u003e爬虫，网络安全最大的威胁之一！ 根据爬取数据类型而分，爬虫有不少种类，比如爬取Email地址的、爬取商品价格的、爬取图片的，而最多的是爬取内容的，内容数据爬虫是为泛滥的！ 爬虫让很多人对其深感苦恼，今天的Node.JS实战，将实现一种防护性能很强的反爬…","created":1580265810,"updated":1580265810,"author":{"avatar_url_template":"https://pic4.zhimg.com/v2-a7f3610ade594ad6fc96a5ea99745e3a.jpg?source=172ae18b","uid":"1090189217656598528","user_type":"people","url_token":"w3sft","id":"3b89f3ee51b6fd7cc6361b189b562be4","description":"","name":"w3sft","is_advertiser":false,"headline":"：）","gender":1,"url":"/people/3b89f3ee51b6fd7cc6361b189b562be4","avatar_url":"https://pic4.zhimg.com/v2-a7f3610ade594ad6fc96a5ea99745e3a_l.jpg?source=172ae18b","is_org":false,"type":"people","badge":[],"badge_v2":{"title":"","merged_badges":[],"detail_badges":[],"icon":"","night_icon":""}},"comment_permission":"all","copyright_permission":"need_review","state":"published","admin_closed_comment":false,"voteup_count":15,"annotation_action":[],"visible_only_to_author":false,"ab_param":{"hb_ans_card":"0"}},"editor_choice":false}]}
